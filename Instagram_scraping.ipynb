{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Instagram scraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yearrypanji/yearry/blob/master/Instagram_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGQh9X-Wu4-7"
      },
      "source": [
        "# 1 Instalasi Program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7SgF4gHU6CI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d5ca49-56ff-43a7-93db-1a2115ad7122"
      },
      "source": [
        "!pip install instagram-scraper"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting instagram-scraper\n",
            "  Downloading https://files.pythonhosted.org/packages/2c/5b/b5eee5adef1077bbc9990c788ba3d9b697d19c36355d49ce11084fd29d87/instagram-scraper-1.9.1.tar.gz\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from instagram-scraper) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from instagram-scraper) (4.41.1)\n",
            "Collecting moviepy>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/54/01a8c4e35c75ca9724d19a7e4de9dc23f0ceb8769102c7de056113af61c3/moviepy-1.0.3.tar.gz (388kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->instagram-scraper) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->instagram-scraper) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->instagram-scraper) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->instagram-scraper) (3.0.4)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.6/dist-packages (from moviepy>=1.0.0->instagram-scraper) (4.4.2)\n",
            "Collecting proglog<=1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/ab/4cb19b578e1364c0b2d6efd6521a8b4b4e5c4ae6528041d31a2a951dd991/proglog-0.1.9.tar.gz\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.6/dist-packages (from moviepy>=1.0.0->instagram-scraper) (1.18.5)\n",
            "Collecting imageio<3.0,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/57/5d899fae74c1752f52869b613a8210a2480e1a69688e65df6cb26117d45d/imageio-2.9.0-py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 13.4MB/s \n",
            "\u001b[?25hCollecting imageio_ffmpeg>=0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/c8/04c6b4a001b8ae7326fb83d6665af1ee58d6cc1acb421f8ea40d2678fe3c/imageio_ffmpeg-0.4.2-py3-none-manylinux2010_x86_64.whl (26.9MB)\n",
            "\u001b[K     |████████████████████████████████| 26.9MB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<3.0,>=2.5->moviepy>=1.0.0->instagram-scraper) (7.0.0)\n",
            "Building wheels for collected packages: instagram-scraper, moviepy, proglog\n",
            "  Building wheel for instagram-scraper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for instagram-scraper: filename=instagram_scraper-1.9.1-cp36-none-any.whl size=35931 sha256=ffda4b0dfd8d433bab471e995ce0a5c104df4a815e8a6453c3657495fbd0639c\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/b5/0a/90c1b539b2f4baa66e816f34a87e8416a7ad7e641adf9377ba\n",
            "  Building wheel for moviepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for moviepy: filename=moviepy-1.0.3-cp36-none-any.whl size=110729 sha256=1db2f2c56ff2f81265ad4ebecc8ae1cd7bc67183b379668bac782e36188bc425\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/fe/1c/f4e6dca9e828d4b979c04e461d7fcc5b8e7bd35f947e665b65\n",
            "  Building wheel for proglog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for proglog: filename=proglog-0.1.9-cp36-none-any.whl size=6149 sha256=6cbe22c7745b222c112667d0e71b2b6103755fdf3cd60ed1d4f17ea1243fe1a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/56/60/1d0306a8d90b188af393c1812ddb502a8821b70917f82dcc00\n",
            "Successfully built instagram-scraper moviepy proglog\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: proglog, imageio, imageio-ffmpeg, moviepy, instagram-scraper\n",
            "  Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "  Found existing installation: moviepy 0.2.3.5\n",
            "    Uninstalling moviepy-0.2.3.5:\n",
            "      Successfully uninstalled moviepy-0.2.3.5\n",
            "Successfully installed imageio-2.9.0 imageio-ffmpeg-0.4.2 instagram-scraper-1.9.1 moviepy-1.0.3 proglog-0.1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0MIRrHwu98I"
      },
      "source": [
        "# 2. Cek Menu Bantuan Program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7uILVGMVJDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e98fd847-99b7-41b5-c2d9-89130274d795"
      },
      "source": [
        "!instagram-scraper --help"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: instagram-scraper [-h] [--destination DESTINATION]\n",
            "                         [--login-user LOGIN_USER] [--login-pass LOGIN_PASS]\n",
            "                         [--followings-input]\n",
            "                         [--followings-output FOLLOWINGS_OUTPUT]\n",
            "                         [--filename FILENAME] [--quiet] [--maximum MAXIMUM]\n",
            "                         [--retain-username] [--media-metadata]\n",
            "                         [--profile-metadata] [--proxies PROXIES]\n",
            "                         [--include-location]\n",
            "                         [--media-types MEDIA_TYPES [MEDIA_TYPES ...]]\n",
            "                         [--latest] [--latest-stamps LATEST_STAMPS]\n",
            "                         [--cookiejar COOKIEJAR] [--tag]\n",
            "                         [--filter [FILTER [FILTER ...]]]\n",
            "                         [--filter-location [FILTER_LOCATION [FILTER_LOCATION ...]]]\n",
            "                         [--filter-location-file FILTER_LOCATION_FILE]\n",
            "                         [--location] [--search-location] [--comments]\n",
            "                         [--no-check-certificate] [--interactive]\n",
            "                         [--retry-forever] [--verbose VERBOSE]\n",
            "                         [--template TEMPLATE]\n",
            "                         [--log_destination LOG_DESTINATION]\n",
            "                         [username [username ...]]\n",
            "\n",
            "instagram-scraper scrapes and downloads an instagram user's photos and videos.\n",
            "\n",
            "positional arguments:\n",
            "  username              Instagram user(s) to scrape\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --destination DESTINATION, -d DESTINATION\n",
            "                        Download destination\n",
            "  --login-user LOGIN_USER, --login_user LOGIN_USER, -u LOGIN_USER\n",
            "                        Instagram login user\n",
            "  --login-pass LOGIN_PASS, --login_pass LOGIN_PASS, -p LOGIN_PASS\n",
            "                        Instagram login password\n",
            "  --followings-input, --followings_input\n",
            "                        Compile list of profiles followed by login-user to use\n",
            "                        as input\n",
            "  --followings-output FOLLOWINGS_OUTPUT, --followings_output FOLLOWINGS_OUTPUT\n",
            "                        Output followings-input to file in destination\n",
            "  --filename FILENAME, -f FILENAME\n",
            "                        Path to a file containing a list of users to scrape\n",
            "  --quiet, -q           Be quiet while scraping\n",
            "  --maximum MAXIMUM, -m MAXIMUM\n",
            "                        Maximum number of items to scrape\n",
            "  --retain-username, --retain_username, -n\n",
            "                        Creates username subdirectory when destination flag is\n",
            "                        set\n",
            "  --media-metadata, --media_metadata\n",
            "                        Save media metadata to json file\n",
            "  --profile-metadata, --profile_metadata\n",
            "                        Save profile metadata to json file\n",
            "  --proxies PROXIES     Enable use of proxies, add a valid JSON with http\n",
            "                        or/and https urls.\n",
            "  --include-location, --include_location\n",
            "                        Include location data when saving media metadata\n",
            "  --media-types MEDIA_TYPES [MEDIA_TYPES ...], --media_types MEDIA_TYPES [MEDIA_TYPES ...], -t MEDIA_TYPES [MEDIA_TYPES ...]\n",
            "                        Specify media types to scrape\n",
            "  --latest              Scrape new media since the last scrape\n",
            "  --latest-stamps LATEST_STAMPS, --latest_stamps LATEST_STAMPS\n",
            "                        Scrape new media since timestamps by user in specified\n",
            "                        file\n",
            "  --cookiejar COOKIEJAR, --cookierjar COOKIEJAR\n",
            "                        File in which to store cookies so that they can be\n",
            "                        reused between runs.\n",
            "  --tag                 Scrape media using a hashtag\n",
            "  --filter [FILTER [FILTER ...]]\n",
            "                        Filter by tags in user posts\n",
            "  --filter-location [FILTER_LOCATION [FILTER_LOCATION ...]]\n",
            "                        filter query by only accepting media with location\n",
            "                        filter as the location id\n",
            "  --filter-location-file FILTER_LOCATION_FILE\n",
            "                        file containing list of locations to filter query by\n",
            "  --location            Scrape media using a location-id\n",
            "  --search-location     Search for locations by name\n",
            "  --comments            Save post comments to json file\n",
            "  --no-check-certificate\n",
            "                        Do not use ssl on transaction\n",
            "  --interactive, -i     Enable interactive login challenge solving\n",
            "  --retry-forever       Retry download attempts endlessly when errors are\n",
            "                        received\n",
            "  --verbose VERBOSE, -v VERBOSE\n",
            "                        Logging verbosity level\n",
            "  --template TEMPLATE, -T TEMPLATE\n",
            "                        Customize filename template\n",
            "  --log_destination LOG_DESTINATION, -l LOG_DESTINATION\n",
            "                        destination folder for the instagram-scraper.log file\n",
            "\n",
            "You can hide your credentials from the history, by reading your\n",
            "username from a local file:\n",
            "\n",
            "$ instagram-scraper @insta_args.txt user_to_scrape\n",
            "\n",
            "with insta_args.txt looking like this:\n",
            "-u=my_username\n",
            "-p=my_password\n",
            "\n",
            "You can add all arguments you want to that file, just remember to have\n",
            "one argument per line.\n",
            "\n",
            "Customize filename:\n",
            "by adding option --template or -T\n",
            "Default is: {urlname}\n",
            "And there are some option:\n",
            "{username}: Instagram user(s) to scrape.\n",
            "{shortcode}: post shortcode, but profile_pic and story are none.\n",
            "{urlname}: filename form url.\n",
            "{mediatype}: type of media.\n",
            "{datetime}: date and time that photo/video post on,\n",
            "             format is: 20180101 01h01m01s\n",
            "{date}: date that photo/video post on,\n",
            "         format is: 20180101\n",
            "{year}: format is: 2018\n",
            "{month}: format is: 01-12\n",
            "{day}: format is: 01-31\n",
            "{h}: hour, format is: 00-23h\n",
            "{m}: minute, format is 00-59m\n",
            "{s}: second, format is 00-59s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9wrpSeLvFYR"
      },
      "source": [
        "# 3 Contoh Scraping Comment dari Post"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H1xkZP5VPTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a5a404-c8e1-459c-8874-f1a979dc29c4"
      },
      "source": [
        "# hanya mengambil comment dan media metadata\n",
        "!instagram-scraper <USERNAME> -u <YOUR_USERNAME(opsional)> -p <YOUR_PASSWORD(opsional)> -m <MAKSIMUM_POST(opsional)> --media-types  none --comments --interactive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 0: `instagram-scraper <bps_statistics> -u <YOUR_USERNAME(opsional)> -p <YOUR_PASSWORD(opsional)> -m <MAKSIMUM_POST(opsional)> --media-types  none --comments --interactive'\n",
            "/bin/bash: @bps_tanjabbar: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9P92PF89dGs"
      },
      "source": [
        "# mengambil comment, metadata, video dan gambar\n",
        "!instagram-scraper <USERNAME> -u <YOUR_USERNAME> -p <YOUR_PASSWORD> -m <MAKSIMUM_POST> --media-types image,video --comments --interactive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1k-1XgmzuuD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f35eb65a-083a-4cc6-f128-ebd80cf4d6e7"
      },
      "source": [
        "# Mengambil media metadata dari sebuah hashtag\n",
        "!instagram-scraper --tag <TAG> -m 30 --media-types none --media-metadata"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Searching giveaway for posts: 8 media [00:03,  2.25 media/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-KGXewCwQdZ"
      },
      "source": [
        "# 4 Contoh Scraping Comment dari Tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATILQEvMwZEK"
      },
      "source": [
        "# hanya mengambil comment dan media metadata\n",
        "!instagram-scraper --tag <TAG> -u <YOUR_USERNAME(opsional)> -p <YOUR_PASSWORD(opsional)> -m <MAKSIMUM_POST(opsional)> --media-types  none --comments --interactive\n",
        "\n",
        "# mengambil comment, metadata, video dan gambar termasuk lokasi\n",
        "!instagram-scraper --tag <TAG> -u <YOUR_USERNAME(opsional)> -p <YOUR_PASSWORD(opsional)> -m <MAKSIMUM_POST(opsional)> --media-types  image,video --comments --interactive --include-location"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGgVg7t59ujB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63983e01-80d7-4365-b713-a9f2479efbc6"
      },
      "source": [
        "!instagram-scraper bibit.id -u rosidiscn -p 26102206 --profile-metadata"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Searching bibit.id for profile pic: 100% 1/1 [00:00<00:00, 1597.22 images/s]\n",
            "Searching bibit.id for stories: 100%|##########| 136/136 [00:00<00:00, 12795.83 media/s]\n",
            "Searching bibit.id for broadcasts: 0 media [00:00, ? media/s]\n",
            "Searching bibit.id for posts: 51 media [00:31, 11.07s/ media]Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 377, in _make_request\n",
            "    httplib_response = conn.getresponse(buffering=True)\n",
            "TypeError: getresponse() got an unexpected keyword argument 'buffering'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/http/client.py\", line 1373, in getresponse\n",
            "    response.begin()\n",
            "  File \"/usr/lib/python3.6/http/client.py\", line 311, in begin\n",
            "    version, status, reason = self._read_status()\n",
            "  File \"/usr/lib/python3.6/http/client.py\", line 272, in _read_status\n",
            "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
            "  File \"/usr/lib/python3.6/socket.py\", line 586, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/lib/python3.6/ssl.py\", line 1012, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "  File \"/usr/lib/python3.6/ssl.py\", line 874, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "  File \"/usr/lib/python3.6/ssl.py\", line 631, in read\n",
            "    v = self._sslobj.read(len, buffer)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/instagram-scraper\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/instagram_scraper/app.py\", line 1629, in main\n",
            "    scraper.scrape()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/instagram_scraper/app.py\", line 661, in scrape\n",
            "    self.get_media(dst, executor, future_to_item, user)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/instagram_scraper/app.py\", line 822, in get_media\n",
            "    unit=' media', disable=self.quiet):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tqdm/std.py\", line 1104, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/instagram_scraper/app.py\", line 985, in query_media_gen\n",
            "    media, end_cursor = self.__query_media(user['id'], end_cursor)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/instagram_scraper/app.py\", line 1002, in __query_media\n",
            "    nodes = self._get_nodes(container)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/instagram_scraper/app.py\", line 573, in _get_nodes\n",
            "    return [self.augment_node(node['node']) for node in container['edges']]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/instagram_scraper/app.py\", line 573, in <listcomp>\n",
            "    return [self.augment_node(node['node']) for node in container['edges']]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/instagram_scraper/app.py\", line 591, in augment_node\n",
            "    details = self.__get_media_details(node['shortcode'])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/instagram_scraper/app.py\", line 607, in __get_media_details\n",
            "    resp = self.get_json(VIEW_MEDIA_URL.format(shortcode))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/instagram_scraper/app.py\", line 230, in get_json\n",
            "    resp = self.safe_get(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/instagram_scraper/app.py\", line 197, in safe_get\n",
            "    response = self.session.get(timeout=CONNECT_TIMEOUT, cookies=self.cookies, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 543, in get\n",
            "    return self.request('GET', url, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 530, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 643, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/adapters.py\", line 449, in send\n",
            "    timeout=timeout\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
            "    chunked=chunked)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 380, in _make_request\n",
            "    httplib_response = conn.getresponse()\n",
            "  File \"/usr/lib/python3.6/http/client.py\", line 1374, in getresponse\n",
            "    except ConnectionError:\n",
            "KeyboardInterrupt\n",
            "Searching bibit.id for posts: 51 media [00:46,  1.10 media/s]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}